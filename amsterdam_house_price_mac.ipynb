{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <ins>Mini-Project: Amsterdam House Price Analysis with Decision Tree Variations and SGD Linear Regression</ins><br>\n",
    "<i> -This notebook is to see as personal exercise-\n",
    "\n",
    "Here we work with the data set \"Amsterdam House Price Prediction\" <i>from Kaggle (https://www.kaggle.com/datasets/thomasnibb/amsterdam-house-price-prediction).</i><br>\n",
    "The overarching goal is to create a model that <b>predicts the market price of houses</b> based on the information we get from the aforementioned dataset. \n",
    "<br>\n",
    "<br>The dataset contains:\n",
    "\n",
    "<ul>\n",
    "<li>address</li>\n",
    "<li><b>zip-code</b></li>\n",
    "<li><b>price</b></li>\n",
    "<li><b>space</b></li>\n",
    "<li><b>room count</b></li>\n",
    "<li>lat/lon coordinates</li>\n",
    "</ul>\n",
    "\n",
    "<br>\n",
    "of <b>924 houses</b> within the amsterdam region. <b>Price</b> will be <b>our target</b> to predict and we will focus on the <b>zip-code, space and room count</b> as <b>our predictors</b><br>\n",
    "<br><i>Note: To plot Amsterdam areas by zip code we will use a second source for the coordinates (https://public.opendatasoft.com/explore/dataset/georef-netherlands-postcode-pc4/export/).</i><br>\n",
    "<br>\n",
    "Since Price is an continues numerical parameter we will use regression models for our predictions. <i>If these wont provide reasonable results we will think about transforming the target price into a categorical parameter which might improve predictions.</i>\n",
    "<br>\n",
    "<br>\n",
    "<i>.... OK! lets get started</i>\n",
    "<br>\n",
    "<br>\n",
    "First we load all packages we will need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading all Packages\n",
    "# General Packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import plotly.express as px\n",
    "import scipy as spy\n",
    "import seaborn as sns\n",
    "import random\n",
    "from unicodedata import category\n",
    "\n",
    "# ML Packages\n",
    "import sklearn as sk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, VotingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import r2_score,mean_squared_error,mean_absolute_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import pickle\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PART I: Exploring the Data\n",
    "We load the .csv (downloaded from Kaggle) to a dataframe and check the data formats and missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading dataframe\n",
    "data = pd.read_csv(\"app/src/HousingPrices-Amsterdam-August-2021.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first look shows us that we have 3 features that are potential good estimators: Rooms, Area (aka house size in mÂ²) and the Zip Code (specifying the location of the house)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First look at data format and check for missing values\n",
    "display(data.head(5))\n",
    "display(data.info())\n",
    "if 0 in data == True:\n",
    "    print(\"Columns contain Zeroes\")\n",
    "else:\n",
    "    print(\"Columns contain NO Zeroes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that, except for the first column, all columns are sensical with representative names. Since \"Unnamed: 0\" appears to be just an index column which we will check by confirming that the values continuously increase by 1 <br>\n",
    "<br>\n",
    "Also we see that only Price is missing values. Since those are only 4 we will exclude the rows entirely "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean of Unnamed and compare to mean of simple continuous series with increase of 1 - indication 1 for Unnamed = Index\n",
    "print(\"Mean of 'Unnamed: 0': \",data[\"Unnamed: 0\"].mean())\n",
    "print(\"Mean of a continuous number series of 'Unnamed: 0' length: \", sum(range(data[\"Unnamed: 0\"].iat[0],data[\"Unnamed: 0\"].iat[-1]+1))/len(data[\"Unnamed: 0\"]))\n",
    "\n",
    "# Calculate slope of Unnamed showing slope is equal 1 - indication 2 for Unnamed = Index\n",
    "test_slope = spy.stats.linregress(data[\"Unnamed: 0\"],data.index)[0]\n",
    "print(\"The slope is: \", test_slope)\n",
    "\n",
    "# Plotting Unnamed to visualize continuity and slope of Unnamed \n",
    "plt.figure(figsize=(3,3))\n",
    "sns.lineplot(data[\"Unnamed: 0\"])\n",
    "plt.title(\"Column is Contineously increasing by 1\")\n",
    "plt.xlabel(\"Pandas Index\")\n",
    "plt.xlim(0,925)\n",
    "plt.ylabel(\"Value\")\n",
    "plt.ylim(0,925)\n",
    "plt.show()\n",
    "\n",
    "# Renaming Unnamed column and drop NaN rows\n",
    "data = data.drop([\"Unnamed: 0\",\"Address\",\"Lon\",\"Lat\"], axis=1)\n",
    "data = data.drop(data.loc[data[\"Price\"].isna() == True,\"Price\"].index)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the mean of \"Unnamed: 0\" is equal to the mean of a simple continuous number series counting from 1 to 925 and the line plot shows straight line with a slop of 1 we conclude that this column is indeed just an index column. The column is therefore renamed to \"index\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking if the size of the house (area) and room count (room) correlate with the house price shows that both can serve as relatively good estimators (room r=0.62; area r=0.83 <br>\n",
    "<br>\n",
    "First we look at a quick correlation matrix and additionally we look at the data with a scatter plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first look at correlations\n",
    "mask = np.array([[1,1,1],[0,1,1],[0,0,1]])\n",
    "sns.heatmap(data[[\"Price\",\"Area\",\"Room\"]].corr(method=\"pearson\"), vmin=0, vmax=1, mask=mask)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig2, axs = plt.subplots(ncols=2)\n",
    "sns.regplot(data=data,x=\"Room\",y=\"Price\", ax=axs[0])\n",
    "sns.regplot(data=data,x=\"Area\",y=\"Price\", ax=axs[1])\n",
    "print(\"r value for Room vs Price: \", spy.stats.linregress(data[\"Room\"], data[\"Price\"])[2])\n",
    "print(\"r value for Area vs Price: \", spy.stats.linregress(data[\"Area\"], data[\"Price\"])[2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets have a look at the zip code data. It is a very appropriate guess that the zip code is a good estimator for the price of a house. This is usally the case of bigger cities like amsterdam is. However when we look up the how zip codes are constructed for the netherlands we see that the code specifies an area down to a street. This is way to specific for the data we have available here. Using this zip codes straight away would dilute the data set, meaning we get for many zip codes only one or two houses to work with. Therefor we first decode the zip to its first 4 numbers which specify a specific area within the city (70 areas)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"Region\"] = data[\"Zip\"].str[0:4].astype(int)\n",
    "data[\"Price\"] = data[\"Price\"].astype(float)\n",
    "data[\"Area\"] = data[\"Area\"].astype(float)\n",
    "data[\"Room\"] = data[\"Room\"].astype(int)\n",
    "data = data.drop([\"Zip\"], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected simply plotting a bar plot representing each region clearly shows that some regions sell for more.<br>\n",
    "<br>\n",
    "<i>Note: Since we are interested in the mean we plot the SEM)</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig3, ax = plt.subplots(figsize=(20,5))\n",
    "fig3 = sns.barplot(data=data,x=\"Region\",y=\"Price\", estimator=\"median\", errorbar=\"se\", color=\"lightblue\")#, order= Price_columns.columns.sort_values())\n",
    "fig3.set_xticklabels(labels=data[\"Region\"].unique() ,fontdict={\"fontsize\":\"8\",\"rotation\":\"vertical\"})\n",
    "fig3.set_title(\"Median Price of each Region (by Zip)\")\n",
    "fig3.set_ylabel(\"Price\")\n",
    "fig3.set_xlabel(\"Region (Zip Code)\")\n",
    "fig3.set(ylim=(0,(2*10**6)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check if there is truly a significant difference within the regions (groups) we perform a quick simple ANOVA test<br>\n",
    "We will see that the p-value is < 0.05, thus we can reject H0 and confirmed that there is a significant difference between the groups<br>\n",
    "<br>\n",
    "<i>Note: For that we first have to quickly transform the data into a list without NaNs so we can use the Scipy function<i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Regio_Price = data[[\"Region\",\"Price\"]].copy()\n",
    "Regio_Price[\"Region\"] = Regio_Price[\"Region\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_list = Regio_Price.Region.unique()\n",
    "All_Regio_Price = []\n",
    "for i in region_list:\n",
    "\n",
    "    All_Regio_Price.append(list(Regio_Price[\"Price\"][Regio_Price[\"Region\"]==i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"p-value of ANOVA test: \", spy.stats.f_oneway(*All_Regio_Price)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a better idea about the regions within amsterdam we us plotly and geojson data to project every region we use onto a map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "geodata=json.load(open(\"app/src/georef-netherlands-postcode-pc4_simple.geojson\",\"r\"))\n",
    "\n",
    "#plotly needs specificly \"id\" within features so we have to copy them from properties to features\n",
    "for i in geodata[\"features\"]:\n",
    "    i[\"id\"]=i[\"properties\"][\"pc4_code\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get zip codes from geojson data\n",
    "zip_list = []\n",
    "for i in geodata[\"features\"]:\n",
    "    zip_list.append(i[\"properties\"][\"pc4_code\"])\n",
    "\n",
    "#check if every zip code in data set is present in geodata\n",
    "for i in data[\"Region\"].unique():\n",
    "    if str(i) not in zip_list:\n",
    "        print(i, \"is not represented in geojson\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_data = data.groupby([\"Region\"]).median().reset_index()[[\"Region\",\"Price\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig2 = px.choropleth_mapbox(\n",
    "    avg_data,\n",
    "    locations=avg_data[\"Region\"],\n",
    "    geojson=geodata,\n",
    "    color=avg_data[\"Price\"],\n",
    "    zoom=9.7, \n",
    "    center = {\"lat\": 52.356157, \"lon\": 4.907736},\n",
    "    color_continuous_scale=\"matter\", \n",
    "    mapbox_style=\"carto-positron\",\n",
    "    hover_data=avg_data,\n",
    "    range_color=[avg_data[\"Price\"].min(),avg_data[\"Price\"].max()],\n",
    "    width=1000,\n",
    "    height=500\n",
    ")\n",
    "\n",
    "fig2.update_layout(                      \n",
    "    margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0}\n",
    ")\n",
    "\n",
    "fig2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_data = (data\n",
    "            .groupby(\"Region\")\n",
    "            .count()\n",
    "            .sort_values(by=\"Price\", ascending=False)\n",
    "            .reset_index())\n",
    "\n",
    "fig3 = plt.subplots(figsize=(10,5))\n",
    "fig3 = sns.barplot(x=agg_data.iloc[:,0], y=agg_data.iloc[:,1], order=agg_data[\"Region\"], color=\"lightblue\")\n",
    "fig3.set_xticklabels(labels=agg_data.iloc[:,0],fontdict={\"fontsize\":\"8\",\"rotation\":\"vertical\"})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When plotting the count of houses per region we realize that many regions represent less than 10 houses. Since we already established that the price in dependent on the region the house is in we will circumvent this high imbalance of data by creating \"new regions\" that correspond to a certain price range<br>\n",
    "Additionally, we will reduce the dimentionality compared to the simple straight forwards approach of simply One-Hot encoding the zip codes.<br>\n",
    "<br>\n",
    "We will arbitrarily create five regions which should represent realistic boundaries: less > 300000, less > 600000, less > 900000, less > 1200000, more < 1200000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PART II: Data Engineering\n",
    "\n",
    "To prepare the data for ML training we have to encode the columns. After preparing One-Hot encoded data set as benchmark we will prepare data sets to simplify the dimensionality of the data. So in total we will have three approaches:\n",
    "<ul>\n",
    "<li><b>One-Hot encoding:</b> Straight forward one-hot encoding as benchmark model.\n",
    "<li><b>Bin Sequence encoding:</b> each zip-region is represented by all new price-regions. Each price-region column holds a 1 if any house of a specific zip-region is within its boundaries. This leads to a 5 digit encoding (e.g. {\"0_to_300k\": <b>1</b>,\"300k_to_600k\": <b>1</b>,\"600k_to_900k\": <b>1</b>,\"900k_to_1200k\": <b>0</b>,\"more_than_1200000\": <b>0</b>})</li>\n",
    "<li><b>Label encoding:</b> each price-regions is encoded into one column (possible because the new price-regions have a meaningful relation). Each zip-region is associated with the most frequent price-region they appear in the data</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One-Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oh_data = data.copy().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = OneHotEncoder(drop=\"first\")\n",
    "cats = pd.DataFrame(enc.fit_transform(oh_data[[\"Region\"]]).toarray())\n",
    "oh_data = pd.concat([oh_data,cats], axis=1).drop([\"index\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating encoding pattern for Sequence Encoding enabling to properly encode possible future data points for predictions\n",
    "encode_pattern_var1 = data.copy().reset_index()\n",
    "\n",
    "enc = OneHotEncoder(drop=\"first\")\n",
    "encode_pattern_var1 = enc.fit(encode_pattern_var1[[\"Region\"]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating  Label Encoding function enabling to properly encode possible future data points for predictions\n",
    "def EncodeRegionVar1(transformData: pd.DataFrame, OH_Encoder: OneHotEncoder, encoderColumn: str) -> pd.DataFrame:\n",
    "    \n",
    "    if \"index\" in transformData.columns:\n",
    "        transformData = transformData.drop([\"index\"], axis=1)\n",
    "        transformData = transformData.reset_index()\n",
    "    \n",
    "    if \"index\" not in transformData.columns:\n",
    "        transformData = transformData.reset_index()\n",
    "    \n",
    "    temp_frame = pd.DataFrame((OH_Encoder.transform(transformData[[encoderColumn]])).toarray())\n",
    "    transformData = pd.concat([transformData,temp_frame], axis=1).drop([encoderColumn], axis=1)\n",
    "    transformData = transformData.drop([\"index\"], axis=1)\n",
    "    transformData.columns = transformData.columns.astype(str)\n",
    "       \n",
    "    return transformData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bin-Sequence Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating bin regions based on prices\n",
    "new_regions_bins = [[0,300000],[300000,600000],[600000,900000],[900000,1200000],[1200000,1000000000]]\n",
    "new_regions = [\"0_to_300k\",\"300k_to_600k\",\"600k_to_900k\",\"900k_to_1200k\",\"more_than_1200000\"]\n",
    "\n",
    "# creating dataframe that contains the actual region of each house sorted into the correct new column\n",
    "df_data = [list(np.where(data[\"Price\"].isin(data.loc[(data[\"Price\"] > new_regions_bins[i][0]) & (data[\"Price\"] <= new_regions_bins[i][1])][\"Price\"])==True,data[\"Region\"].astype(int),0)) for i in range(0,len(new_regions_bins))]\n",
    "temp_frame = pd.DataFrame(data=dict(zip(new_regions,df_data)), dtype=\"int32\")\n",
    "\n",
    "# add the new dataframe to the data\n",
    "data = pd.concat([data.reset_index(),temp_frame], axis=1).drop([\"index\"], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the count of houses in each regions shows a big improvement as the lowest count is now 70. However, we also see that the data is still very imbalanced.<br>\n",
    "<br>\n",
    "<b>Since the data set is so small we do not have a very good angle to work on the data imbalance. Over-sampling is extremely likely to lead to over fitting. Under-sampling is also no option with this small data set. For the purpose of this exercise we will just continue with the data as is.</b><br>\n",
    "<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting new price-regions vs house count within the region\n",
    "fig4, ax = plt.subplots(figsize=(10,5))\n",
    "fig4 = sns.barplot(data = (pd.DataFrame(data[new_regions][data[new_regions] != 0].count()).transpose()),color=\"lightblue\", errorbar=\"sd\")\n",
    "fig4.set_title(\"Number of Houses per Price Category\")\n",
    "fig4.set_ylabel(\"Count\")\n",
    "fig4.set_xlabel(\"Price Category\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label encoding\n",
    "for t,i in enumerate(new_regions):\n",
    "    data[i] = np.where(data[i] == 0,0,t+1)\n",
    "\n",
    "data[\"price_cat_encoded\"] = data[new_regions].sum(axis=1)\n",
    "data = data.drop(new_regions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating encoding pattern for Bin Sequence Encoding enabling to properly encode possible future data points for predictions\n",
    "encode_pattern_var2 = data.copy()\n",
    "\n",
    "region_labels_collection = {i: [temp_frame[i].unique()] for i in new_regions}\n",
    "\n",
    "for i in region_labels_collection:\n",
    "    encode_pattern_var2.loc[encode_pattern_var2[\"Region\"].isin(*region_labels_collection[i]) == True, i] = 1\n",
    "\n",
    "encode_pattern_var2 = encode_pattern_var2.drop([\"Price\", \"Area\", \"Room\",\"price_cat_encoded\"], axis=1).drop_duplicates(subset=\"Region\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating Bin Sequence Encoding function enabling to properly encode also possible future data points for predictions\n",
    "def EncodeRegionVar2(transformData: pd.DataFrame, mergeColumn: str, encodePattern: pd.DataFrame, encoderColumns: list) -> pd.DataFrame:\n",
    "    transformData = pd.merge(transformData,encodePattern,how=\"left\", on=mergeColumn)\n",
    "    \n",
    "    for i in [\"Region\",\"price_cat_encoded\"]:\n",
    "        if i in transformData.columns:\n",
    "            transformData = transformData.drop(i, axis=1)\n",
    "            \n",
    "    transformData[encoderColumns] = transformData[encoderColumns].fillna(0)\n",
    "    return transformData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label Encoding (Region by frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating encoding pattern for Label Encoding enabling to properly encode possible future data points for predictions\n",
    "encode_pattern_var3 = data.copy()\n",
    "\n",
    "encode_pattern_var3 = [(encode_pattern_var3\n",
    "                    .query(\"Region == @i\")\n",
    "                    .groupby(\"price_cat_encoded\")\n",
    "                    .count()\n",
    "                    .idxmax()[\"Region\"]) \n",
    "                    \n",
    "                    for i in encode_pattern_var3[\"Region\"].unique()]\n",
    "\n",
    "encode_pattern_var3 = pd.DataFrame({\"Region\":list(data[\"Region\"].unique()),\"most_frequent_cat\":encode_pattern_var3})\n",
    "encode_pattern_var3[\"most_frequent_cat\"] = encode_pattern_var3[\"most_frequent_cat\"].astype(\"category\")\n",
    "\n",
    "# translate the numeric label to the actual label just for later plotting\n",
    "temp_for_plot = encode_pattern_var3.copy()\n",
    "temp_for_plot[\"most_frequent_cat\"] = temp_for_plot[\"most_frequent_cat\"].replace([1,2,3,4,5],new_regions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating  Label Encoding function enabling to properly encode also possible future data points for predictions\n",
    "def EncodeRegionVar3(transformData: pd.DataFrame, NewColumnName: str, encodePattern: pd.DataFrame, encoderColumn: str) -> pd.DataFrame:\n",
    "    transformData[\"temp_col1\"] = [int(encodePattern.query(\"Region == @i\")[encoderColumn]) for i in transformData[\"Region\"]]\n",
    "    transformData = transformData.drop([NewColumnName,\"Region\"], axis=1).rename(columns={\"temp_col1\" : NewColumnName})\n",
    "    return transformData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again we take a look at the new price-regions (bins) projected on a map of Amsterdam. We clearly see that most areas oe city have the majority of houses prices at 300k to 600k. This is true for more suburban as well as central areas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the newly created price-regions reflected by the most frequent price within each zip-region\n",
    "fig2 = px.choropleth_mapbox(\n",
    "    temp_for_plot,\n",
    "    locations=temp_for_plot[\"Region\"],\n",
    "    geojson=geodata,\n",
    "    color=temp_for_plot[\"most_frequent_cat\"],\n",
    "    zoom=9.7, \n",
    "    center = {\"lat\": 52.356157, \"lon\": 4.907736},\n",
    "    category_orders={\"most_frequent_cat\":new_regions},\n",
    "    color_discrete_sequence = [\"#fce6aa\",\"#f08e62\",\"#c53a59\",\"#781a60\",\"#282828\"],\n",
    "    mapbox_style=\"carto-positron\",\n",
    "    hover_data=[temp_for_plot[\"Region\"],temp_for_plot[\"most_frequent_cat\"]],\n",
    "    width=1000,\n",
    "    height=500\n",
    ")\n",
    "\n",
    "fig2.update_layout(                      \n",
    "    margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0},\n",
    "    legend_title_text=\"Most Frequent Category within a Region\"\n",
    "\n",
    ")\n",
    "\n",
    "fig2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PART II: Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we prepare the data by splitting with stratifying (Regions), normal scaling and using the encoding functions.<br>\n",
    "<br>\n",
    "We will use a simple SGD Regression as well as three types of decision trees (Random Forest, AdaBoosted and XGBoosted).<br>\n",
    "<br>\n",
    "Finally we will create an assemble with all 4 the get our final predictions. We will estimate the performance of the estimation with the R2 and MSE values. To have a more intuitive understanding of the error we will also look at the MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling\n",
    "scaler = StandardScaler()\n",
    "data_scaled = data[[\"Price\",\"Area\",\"Room\",\"Region\",\"price_cat_encoded\"]].copy()\n",
    "X_to_scale = [\"Area\", \"Room\"]\n",
    "data_scaled[X_to_scale] = pd.DataFrame(scaler.fit_transform(data[X_to_scale]), columns=X_to_scale)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting\n",
    "rseed = random.seed(10)\n",
    "X_train, x_test, Y_train, y_test = train_test_split(data_scaled[data_scaled.columns[1:]], \n",
    "                                                    data_scaled[\"Price\"], \n",
    "                                                    test_size=0.15, \n",
    "                                                    train_size=0.85, \n",
    "                                                    random_state=rseed, \n",
    "                                                    shuffle=True, \n",
    "                                                    stratify=data[\"price_cat_encoded\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding variant 1: One-Hot encoding\n",
    "X_train_1 = EncodeRegionVar1(X_train,encode_pattern_var1,\"Region\").drop([\"price_cat_encoded\"], axis=1)\n",
    "x_test_1 = EncodeRegionVar1(x_test,encode_pattern_var1,\"Region\").drop([\"price_cat_encoded\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding variant 2: Bin Sequence encoding\n",
    "X_train_2 = EncodeRegionVar2(X_train,\"Region\",encode_pattern_var2,  new_regions)\n",
    "x_test_2 = EncodeRegionVar2(x_test,\"Region\",encode_pattern_var2,  new_regions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding variant 3: label encoding\n",
    "X_train_3 = EncodeRegionVar3(X_train,\"price_cat_encoded\",encode_pattern_var3,\"most_frequent_cat\")\n",
    "x_test_3 = EncodeRegionVar3(x_test,\"price_cat_encoded\",encode_pattern_var3,\"most_frequent_cat\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SGD modelling\n",
    "param_grid_SGD = {\"loss\" : [\"squared_error\"],\n",
    "              \"alpha\" : [0.0001, 0.001, 0.01, 0.1, 0.2],\n",
    "              \"penalty\" : [\"l2\", \"l1\", \"elasticnet\", \"none\"],\n",
    "              \"eta0\" : [0.0001,0.001,0.01,0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "                \"max_iter\" : [2000,10000,20000]\n",
    "             }\n",
    "\n",
    "model_SGD = GridSearchCV(SGDRegressor(random_state=rseed), param_grid_SGD ,cv=10, verbose=1, n_jobs=-1)\n",
    "\n",
    "model_SGD_mono = model_SGD.fit(X_train_1,Y_train)\n",
    "print(model_SGD_mono.best_estimator_)\n",
    "\n",
    "\n",
    "y_pred = model_SGD_mono.predict(x_test_1)\n",
    "\n",
    "print(\"R2: \", r2_score(y_test,y_pred))\n",
    "print(\"MSE: \", mean_squared_error(y_test,y_pred))\n",
    "print(\"MAE: \", mean_absolute_error(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RandomForest modelling\n",
    "param_grid_RanFor = {\"criterion\" : [\"squared_error\",\"absolute_error\"],\n",
    "              \"min_samples_split\" : [2,3,4,5,6],\n",
    "              \"min_samples_leaf\" : [2,3,4,5,6],\n",
    "              \"min_impurity_decrease\" : [0,0.01,0.25,0.5]\n",
    "             }\n",
    "\n",
    "model_RanFor = GridSearchCV(RandomForestRegressor(random_state=rseed), param_grid_RanFor, cv=20, verbose = 1, n_jobs=-1)\n",
    "\n",
    "model_RanFor_mono = model_RanFor.fit(X_train_1,Y_train)\n",
    "print(model_RanFor_mono.best_estimator_)\n",
    "\n",
    "\n",
    "y_pred = model_RanFor_mono.predict(x_test_1)\n",
    "\n",
    "print(\"R2: \", r2_score(y_test,y_pred))\n",
    "print(\"MSE: \", mean_squared_error(y_test,y_pred))\n",
    "print(\"MAE: \", mean_absolute_error(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AdaBoost modelling\n",
    "param_grid_Ada = {\"loss\" : [\"linear\",\"square\", \"exponential\"],\n",
    "              \"n_estimators\" : [50,100,500,1000],\n",
    "              \"learning_rate\" : [0.1,0.01,0.001,0.0001]\n",
    "             }\n",
    "\n",
    "model_Ada = GridSearchCV(AdaBoostRegressor(random_state=rseed, base_estimator=None), param_grid_Ada, cv=20, verbose=1, n_jobs=-1)\n",
    "\n",
    "model_Ada_mono = model_Ada.fit(X_train_1,Y_train)\n",
    "print(model_Ada_mono.best_estimator_)\n",
    "\n",
    "\n",
    "y_pred = model_Ada_mono.predict(x_test_1)\n",
    "\n",
    "print(\"R2: \", r2_score(y_test,y_pred))\n",
    "print(\"MSE: \", mean_squared_error(y_test,y_pred))\n",
    "print(\"MAE: \", mean_absolute_error(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost modelling\n",
    "param_grid_XGB = {\"n_estimators\" : [500,750,1000,2000,5000],\n",
    "              \"learning_rate\" : [0.01,0.001,0.0001],\n",
    "              \"max_depth\" : [1,2,3,4],\n",
    "              \"booster\" : [\"gbtree\", \"gblinear\"]\n",
    "             }\n",
    "\n",
    "model_XGB = GridSearchCV(xgb.XGBRegressor(random_state=rseed), param_grid_XGB, cv=20, verbose=1, n_jobs=-1)\n",
    "\n",
    "model_XGB_mono = model_XGB.fit(X_train_1,Y_train)\n",
    "print(model_XGB_mono.best_estimator_)\n",
    "\n",
    "\n",
    "y_pred = model_XGB_mono.predict(x_test_1)\n",
    "\n",
    "print(\"R2: \", r2_score(y_test,y_pred))\n",
    "print(\"MSE: \", mean_squared_error(y_test,y_pred))\n",
    "print(\"MAE: \", mean_absolute_error(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Voting Regressor Ensemble of all 4 model types - One-Hot Encoded - Variant 1\n",
    "model_Vote_1 = VotingRegressor([('model_SGF', model_SGD), ('model_RanFor', model_RanFor), ('model_Ada', model_Ada), ('model_XGB', model_XGB)])\n",
    "\n",
    "model_Vote_1.fit(X_train_1,Y_train)\n",
    "\n",
    "y_pred_1 = model_Vote_1.predict(x_test_1)\n",
    "\n",
    "print(\"---One-Hot-Encoding---\")\n",
    "print(\"R2: \", r2_score(y_test,y_pred_1))\n",
    "print(\"MSE: \", mean_squared_error(y_test,y_pred_1))\n",
    "print(\"MAE: \", mean_absolute_error(y_test,y_pred_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Voting Regressor Ensemble of all 4 model types - Bin Sequence Encoded - Variant 2\n",
    "model_Vote_2 = VotingRegressor([('model_SGF', model_SGD), ('model_RanFor', model_RanFor), ('model_Ada', model_Ada), ('model_XGB', model_XGB)])\n",
    "\n",
    "model_Vote_2.fit(X_train_2,Y_train)\n",
    "\n",
    "y_pred_2 = model_Vote_2.predict(x_test_2)\n",
    "\n",
    "print(\"---Sequence Encoded---\")\n",
    "print(\"R2: \", r2_score(y_test,y_pred_2))\n",
    "print(\"MSE: \", mean_squared_error(y_test,y_pred_2))\n",
    "print(\"MAE: \", mean_absolute_error(y_test,y_pred_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Voting Regressor Ensemble of all 4 model types - Label Encoded - Variant 3\n",
    "model_Vote_3 = VotingRegressor([('model_SGF', model_SGD), ('model_RanFor', model_RanFor), ('model_Ada', model_Ada), ('model_XGB', model_XGB)])\n",
    "\n",
    "model_Vote_3.fit(X_train_3,Y_train)\n",
    "\n",
    "y_pred_3 = model_Vote_3.predict(x_test_3)\n",
    "\n",
    "print(\"---Label Encoded---\")\n",
    "print(\"R2: \", r2_score(y_test,y_pred_3))\n",
    "print(\"MSE: \", mean_squared_error(y_test,y_pred_3))\n",
    "print(\"MAE: \", mean_absolute_error(y_test,y_pred_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"one_hot_enc_model.sav\"\n",
    "pickle.dump(model_Vote_1, open(filename, 'wb'))\n",
    "\n",
    "filename = \"one_hot_enc.sav\"\n",
    "pickle.dump(enc, open(filename, 'wb'))\n",
    "\n",
    "filename = \"sequence_enc_model.sav\"\n",
    "pickle.dump(model_Vote_2, open(filename, 'wb'))\n",
    "\n",
    "filename = \"lable_enc_model.sav\"\n",
    "pickle.dump(model_Vote_3, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <b>Final Thoughts</b>\n",
    "We have seen that the features of the data set, although few, are good estimator for the house price. However, the imbalance of the data set (specifically of the categorical feature) is a critical issue that should be stressed in a real scenario. Since the data set is so small we have not much wiggle room to combat this issue.<br>\n",
    "The performance of the models are not the worst. The Sequence Encoding of the zip-regions performs a little better. However, it is unlikely that somebody in real life would be happy with a error margin of about 130000. Additionally, although we use a grid search that includes a K-fold of 10 the performance might vary after every training. We are in the same situation when looking at the data splitting. The representation of the data might highly vary from one split to the next. To combat this we could go on and create and train different sets of splits however for the purpose of this notebook and the low potential gain we conclude this exercise at this point"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "ec0b995a1df0c33db27325d64eec6c058cf9b85603c63310ec86287631d16129"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
